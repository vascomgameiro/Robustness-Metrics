{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.pytorch_trainer import PyTorchTrainer\n",
    "from src.data_loader import dataloader\n",
    "import src.model_constructor as constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\data_loader.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_tiny = torch.load(path_tiny)\n",
      "c:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\data_loader.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_r = torch.load(path_r)\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "processed_dir = Path(\"data/processed/\")\n",
    "model_dir = Path(\"models/\")\n",
    "plots_path = Path(\"reports/figures/\")\n",
    "predictions_path = Path(\"data/predictions/\")\n",
    "\n",
    "# Dataset paths\n",
    "path_r = processed_dir / \"r.pt\"\n",
    "path_tiny = processed_dir / \"tiny.pt\"\n",
    "\n",
    "# Load datasets\n",
    "train_loader, val_loader, test_loader_tiny, test_loader_r, test_tiny= dataloader(path_tiny, path_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of 32 models generated!!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def models_iterator(depths, filters_sizes, optimizers, drops, lrs):\n",
    "    models_to_train = []\n",
    "    for depth in depths:\n",
    "        fs = filters_sizes[str(depth)]\n",
    "        d = drops[str(depth)]\n",
    "        configurations = list(itertools.product(fs, optimizers, d, lrs))\n",
    "\n",
    "        for config in configurations:\n",
    "            filters_size, optimizer, drop, lr = config\n",
    "\n",
    "            nr_filters = filters_size[:depth]\n",
    "            conv_layers = constructor.Conv(nr_conv=depth, nr_filters=nr_filters, maxpool_batchnorm=True)\n",
    "            fc_size = filters_size[depth:]\n",
    "            act_fun = [\"ReLU\"] * depth\n",
    "            dropouts = drop\n",
    "            fc_layers = constructor.FC(\n",
    "                nr_fc=depth,\n",
    "                fc_size=fc_size,\n",
    "                act_funs=act_fun,\n",
    "                dropouts=dropouts,\n",
    "                in_features=conv_layers.finaldim,\n",
    "                num_classes=62,\n",
    "                batchnorm=True,\n",
    "            )\n",
    "\n",
    "            # Create the model using the CNN constructor\n",
    "            model = constructor.CNN(\n",
    "                conv_layers=conv_layers, fc_layers=fc_layers, num_classes=62, lr=lr, optim=optimizer\n",
    "            )  # Assuming 62 classes as an example\n",
    "\n",
    "            # Store the model and its parameters in the list\n",
    "            model_info = {\"name\": f\"{model.name}\", \"model\": model, \"params\": {\"lr\": lr, \"optimizer\": optimizer}}\n",
    "\n",
    "            models_to_train.append(model_info)\n",
    "\n",
    "    return models_to_train\n",
    "\n",
    "\n",
    "depths = [2, 4]\n",
    "filters_sizes = {\n",
    "    \"2\": [[8, 16, 160, 80], [32, 64, 320, 160]],\n",
    "    \"4\": [[4, 8, 16, 32, 200, 200, 160, 80], [8, 16, 32, 64, 400, 320, 160, 80]],\n",
    "}\n",
    "lrs = [0.01, 0.001]\n",
    "drops = {\"2\": [[0.0, 0.0], [0.5, 0.2]], \"4\": [[0.0] * 4, [0.5, 0.3, 0.3, 0.2]]}\n",
    "optimizers = [\"adam\", \"sgd\"]\n",
    "\n",
    "models_to_train = models_iterator(depths, filters_sizes, optimizers, drops, lrs)\n",
    "print(f\"list of {len(models_to_train)} models generated!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = models_to_train[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Training...\n",
      "\n",
      "Epoch [1/100]: Train Loss: 3.9387, Train Acc: 4.72% | Val Loss: 4.0336, Val Acc: 5.28%\n",
      "New best model found! Validation Accuracy: 5.28%\n",
      "Epoch [2/100]: Train Loss: 3.7848, Train Acc: 7.05% | Val Loss: 3.8451, Val Acc: 5.79%\n",
      "New best model found! Validation Accuracy: 5.79%\n",
      "Epoch [3/100]: Train Loss: 3.6987, Train Acc: 8.33% | Val Loss: 3.7538, Val Acc: 8.58%\n",
      "New best model found! Validation Accuracy: 8.58%\n",
      "Epoch [4/100]: Train Loss: 3.6298, Train Acc: 9.63% | Val Loss: 3.8146, Val Acc: 8.43%\n",
      "Epoch [5/100]: Train Loss: 3.5483, Train Acc: 11.46% | Val Loss: 3.7133, Val Acc: 8.47%\n",
      "Epoch [6/100]: Train Loss: 3.4878, Train Acc: 12.57% | Val Loss: 3.5338, Val Acc: 11.88%\n",
      "New best model found! Validation Accuracy: 11.88%\n",
      "Epoch [7/100]: Train Loss: 3.4360, Train Acc: 13.43% | Val Loss: 3.6128, Val Acc: 10.63%\n",
      "Epoch [8/100]: Train Loss: 3.3972, Train Acc: 14.51% | Val Loss: 3.5496, Val Acc: 11.80%\n",
      "Epoch [9/100]: Train Loss: 3.3587, Train Acc: 15.04% | Val Loss: 3.4852, Val Acc: 14.11%\n",
      "New best model found! Validation Accuracy: 14.11%\n",
      "Epoch [10/100]: Train Loss: 3.3222, Train Acc: 15.89% | Val Loss: 3.4053, Val Acc: 15.18%\n",
      "New best model found! Validation Accuracy: 15.18%\n",
      "Epoch [11/100]: Train Loss: 3.2889, Train Acc: 16.66% | Val Loss: 3.5180, Val Acc: 13.09%\n",
      "Epoch [12/100]: Train Loss: 3.2530, Train Acc: 17.35% | Val Loss: 3.5011, Val Acc: 13.34%\n",
      "Epoch [13/100]: Train Loss: 3.2283, Train Acc: 17.75% | Val Loss: 3.3878, Val Acc: 16.35%\n",
      "New best model found! Validation Accuracy: 16.35%\n",
      "Epoch [14/100]: Train Loss: 3.1931, Train Acc: 18.26% | Val Loss: 3.4875, Val Acc: 13.71%\n",
      "Epoch [15/100]: Train Loss: 3.1567, Train Acc: 19.43% | Val Loss: 3.4183, Val Acc: 15.54%\n",
      "Epoch [16/100]: Train Loss: 3.1230, Train Acc: 19.65% | Val Loss: 3.3723, Val Acc: 16.28%\n",
      "Epoch [17/100]: Train Loss: 3.0921, Train Acc: 20.38% | Val Loss: 3.3933, Val Acc: 15.51%\n",
      "Epoch [18/100]: Train Loss: 3.0469, Train Acc: 20.92% | Val Loss: 3.4384, Val Acc: 16.31%\n",
      "Epoch [19/100]: Train Loss: 3.0159, Train Acc: 21.77% | Val Loss: 3.3842, Val Acc: 16.64%\n",
      "New best model found! Validation Accuracy: 16.64%\n",
      "Epoch [20/100]: Train Loss: 2.8199, Train Acc: 25.59% | Val Loss: 3.3393, Val Acc: 17.52%\n",
      "New best model found! Validation Accuracy: 17.52%\n",
      "Epoch [21/100]: Train Loss: 2.7568, Train Acc: 26.85% | Val Loss: 3.3674, Val Acc: 18.11%\n",
      "New best model found! Validation Accuracy: 18.11%\n",
      "Epoch [22/100]: Train Loss: 2.7259, Train Acc: 28.00% | Val Loss: 3.3978, Val Acc: 17.41%\n",
      "Epoch [23/100]: Train Loss: 2.7182, Train Acc: 27.50% | Val Loss: 3.4005, Val Acc: 18.04%\n",
      "Epoch [24/100]: Train Loss: 2.6816, Train Acc: 28.32% | Val Loss: 3.4121, Val Acc: 17.78%\n",
      "Epoch [25/100]: Train Loss: 2.6725, Train Acc: 28.91% | Val Loss: 3.4180, Val Acc: 17.38%\n",
      "Epoch [26/100]: Train Loss: 2.6703, Train Acc: 28.78% | Val Loss: 3.4272, Val Acc: 17.85%\n",
      "Epoch [27/100]: Train Loss: 2.6631, Train Acc: 29.01% | Val Loss: 3.4263, Val Acc: 17.52%\n",
      "Epoch [28/100]: Train Loss: 2.6571, Train Acc: 28.42% | Val Loss: 3.4094, Val Acc: 17.34%\n",
      "Epoch [29/100]: Train Loss: 2.6712, Train Acc: 28.66% | Val Loss: 3.4141, Val Acc: 17.63%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import copy\n",
    "\n",
    "model = config[\"model\"].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = config[\"name\"]\n",
    "optimizer = config[\"params\"][\"optimizer\"]\n",
    "\n",
    "untrained = copy.deepcopy(model)\n",
    "\n",
    "path_to_model = f\"models/{model_name}\"\n",
    "path_to_predictions = os.path.join(path_to_model, \"predictions\")\n",
    "path_to_plots = os.path.join(path_to_model, \"plots\")\n",
    "\n",
    "\n",
    "optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
    "optim_cls = optims[optimizer]\n",
    "\n",
    "os.makedirs(path_to_model, exist_ok=True)\n",
    "os.makedirs(path_to_plots, exist_ok=True)\n",
    "os.makedirs(path_to_predictions, exist_ok=True)\n",
    "\n",
    "optimizer = optim_cls(model.parameters(), lr=model.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "trainer = PyTorchTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), f\"{path_to_model}/untrained.pt\")\n",
    "\n",
    "trainer.train(num_epochs=100, early_stopping_patience= 10)\n",
    "model = trainer.best_model\n",
    "trainer.save_best_model(path_to_model)\n",
    "trainer.save_plots(path_to_plots)\n",
    "\n",
    "logits_r, labels_r = trainer.predict(test_loader_r)\n",
    "trainer.save_predictions(logits_r, f\"{path_to_predictions}/r.npy\")\n",
    "logits_tiny, labels_tiny = trainer.predict(test_loader_tiny)\n",
    "trainer.save_predictions(logits_tiny, f\"{path_to_predictions}/tiny.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the untrained model\n",
    "untrained_model_path = model_dir / f\"{model_name}_untrained.pt\"\n",
    "torch.save(model.state_dict(), untrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "\n",
      "Epoch [1/100]: Train Loss: 3.9994, Train Acc: 3.87% | Val Loss: 3.8363, Val Acc: 6.20%\n",
      "New best model found! Validation Accuracy: 6.20%\n",
      "Epoch [2/100]: Train Loss: 3.8588, Train Acc: 5.62% | Val Loss: 3.7587, Val Acc: 7.40%\n",
      "New best model found! Validation Accuracy: 7.40%\n",
      "Epoch [3/100]: Train Loss: 3.8023, Train Acc: 6.61% | Val Loss: 3.7219, Val Acc: 8.28%\n",
      "New best model found! Validation Accuracy: 8.28%\n",
      "Epoch [4/100]: Train Loss: 3.7562, Train Acc: 7.50% | Val Loss: 3.6751, Val Acc: 9.86%\n",
      "New best model found! Validation Accuracy: 9.86%\n",
      "Epoch [5/100]: Train Loss: 3.7055, Train Acc: 8.53% | Val Loss: 3.6343, Val Acc: 10.37%\n",
      "New best model found! Validation Accuracy: 10.37%\n",
      "Epoch [6/100]: Train Loss: 3.6634, Train Acc: 9.08% | Val Loss: 3.6652, Val Acc: 10.12%\n",
      "Epoch [7/100]: Train Loss: 3.6250, Train Acc: 10.08% | Val Loss: 3.6209, Val Acc: 11.29%\n",
      "New best model found! Validation Accuracy: 11.29%\n",
      "Epoch [8/100]: Train Loss: 3.5942, Train Acc: 10.74% | Val Loss: 3.6161, Val Acc: 11.80%\n",
      "New best model found! Validation Accuracy: 11.80%\n",
      "Epoch [9/100]: Train Loss: 3.5590, Train Acc: 11.35% | Val Loss: 3.5436, Val Acc: 13.09%\n",
      "New best model found! Validation Accuracy: 13.09%\n",
      "Epoch [10/100]: Train Loss: 3.5256, Train Acc: 12.47% | Val Loss: 3.4525, Val Acc: 14.66%\n",
      "New best model found! Validation Accuracy: 14.66%\n",
      "Epoch [11/100]: Train Loss: 3.4910, Train Acc: 12.72% | Val Loss: 3.4534, Val Acc: 14.81%\n",
      "New best model found! Validation Accuracy: 14.81%\n",
      "Epoch [12/100]: Train Loss: 3.4651, Train Acc: 13.00% | Val Loss: 3.4483, Val Acc: 14.55%\n",
      "Epoch [13/100]: Train Loss: 3.4371, Train Acc: 13.85% | Val Loss: 3.4017, Val Acc: 14.66%\n",
      "Epoch [14/100]: Train Loss: 3.4035, Train Acc: 14.34% | Val Loss: 3.4012, Val Acc: 14.44%\n",
      "Epoch [15/100]: Train Loss: 3.3847, Train Acc: 14.48% | Val Loss: 3.3694, Val Acc: 15.95%\n",
      "New best model found! Validation Accuracy: 15.95%\n",
      "Epoch [16/100]: Train Loss: 3.3459, Train Acc: 15.41% | Val Loss: 3.3344, Val Acc: 16.35%\n",
      "New best model found! Validation Accuracy: 16.35%\n",
      "Epoch [17/100]: Train Loss: 3.3113, Train Acc: 15.87% | Val Loss: 3.4055, Val Acc: 15.32%\n",
      "Epoch [18/100]: Train Loss: 3.2849, Train Acc: 16.36% | Val Loss: 3.3351, Val Acc: 16.72%\n",
      "New best model found! Validation Accuracy: 16.72%\n",
      "Epoch [19/100]: Train Loss: 3.2513, Train Acc: 17.10% | Val Loss: 3.3076, Val Acc: 17.38%\n",
      "New best model found! Validation Accuracy: 17.38%\n",
      "Epoch [20/100]: Train Loss: 3.2123, Train Acc: 17.82% | Val Loss: 3.2373, Val Acc: 18.66%\n",
      "New best model found! Validation Accuracy: 18.66%\n",
      "Epoch [21/100]: Train Loss: 3.1916, Train Acc: 18.09% | Val Loss: 3.2800, Val Acc: 18.37%\n",
      "Epoch [22/100]: Train Loss: 3.1589, Train Acc: 18.47% | Val Loss: 3.2984, Val Acc: 16.61%\n",
      "Epoch [23/100]: Train Loss: 3.1322, Train Acc: 18.97% | Val Loss: 3.2741, Val Acc: 16.94%\n",
      "Epoch [24/100]: Train Loss: 3.0079, Train Acc: 21.15% | Val Loss: 3.1766, Val Acc: 19.46%\n",
      "New best model found! Validation Accuracy: 19.46%\n",
      "Epoch [25/100]: Train Loss: 2.9762, Train Acc: 21.70% | Val Loss: 3.1748, Val Acc: 19.02%\n",
      "Epoch [26/100]: Train Loss: 2.9638, Train Acc: 22.16% | Val Loss: 3.1727, Val Acc: 19.13%\n",
      "Epoch [27/100]: Train Loss: 2.9483, Train Acc: 22.05% | Val Loss: 3.1711, Val Acc: 18.48%\n",
      "Epoch [28/100]: Train Loss: 2.9329, Train Acc: 22.41% | Val Loss: 3.1666, Val Acc: 19.21%\n",
      "Epoch [29/100]: Train Loss: 2.9292, Train Acc: 22.28% | Val Loss: 3.1682, Val Acc: 18.70%\n",
      "Epoch [30/100]: Train Loss: 2.9205, Train Acc: 22.54% | Val Loss: 3.1785, Val Acc: 18.88%\n",
      "Epoch [31/100]: Train Loss: 2.9072, Train Acc: 23.02% | Val Loss: 3.1867, Val Acc: 18.66%\n",
      "Epoch [32/100]: Train Loss: 2.8899, Train Acc: 23.41% | Val Loss: 3.1633, Val Acc: 19.32%\n",
      "Epoch [33/100]: Train Loss: 2.8967, Train Acc: 23.32% | Val Loss: 3.1689, Val Acc: 18.99%\n",
      "Epoch [34/100]: Train Loss: 2.8854, Train Acc: 23.31% | Val Loss: 3.1593, Val Acc: 19.32%\n",
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train(num_epochs=100, early_stopping_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to models\\decent.pt\n",
      "Predictions saved to data\\predictions\\decent_r.npy\n",
      "Loss plot saved to reports\\figures_loss.png\n",
      "Accuracy plot saved to reports\\figures_accuracy.png\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trained_model_path = model_dir / f\"{model_name}.pt\"\n",
    "trainer.save_best_model(trained_model_path)\n",
    "\n",
    "# Save predictions\n",
    "predictions_path_r = predictions_path / f\"{model_name}_r.npy\"\n",
    "predictions = trainer.predict(test_loader_r)\n",
    "trainer.save_predictions(predictions, predictions_path_r)\n",
    "\n",
    "# Save training plots\n",
    "trainer.save_plots(plots_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\AppData\\Local\\Temp\\ipykernel_2660\\323897489.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  untrained_model.load_state_dict(torch.load(untrained_model_path))\n",
      "C:\\Users\\vasco\\AppData\\Local\\Temp\\ipykernel_2660\\323897489.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model.load_state_dict(torch.load(trained_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model = constructor.CNN(conv_layers=decent_conv, fc_layers=decent_fc, num_classes=62, lr=10, optim=optimizer)\n",
    "trained_model = constructor.CNN(conv_layers=decent_conv, fc_layers=decent_fc, num_classes=62, lr=10, optim=optimizer)\n",
    "\n",
    "\n",
    "untrained_model.load_state_dict(torch.load(untrained_model_path))\n",
    "trained_model.load_state_dict(torch.load(trained_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=150, out_features=62, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=4096, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=150, out_features=62, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3312153796024764"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perturbed_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    batch_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            batch_correct += pred.eq(target).sum().item()\n",
    "    perturbed_accuracy = batch_correct / len(dataloader.dataset)\n",
    "    return perturbed_accuracy\n",
    "\n",
    "calculate_perturbed_accuracy(trained_model, train_loader, 'cuda'if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=4096, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=150, out_features=62, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasures_sharpeness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_pac_bayes_metrics\n\u001b[1;32m----> 4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_pac_bayes_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muntrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\measures_sharpeness.py:297\u001b[0m, in \u001b[0;36mcalculate_pac_bayes_metrics\u001b[1;34m(model, init_model, dataloader, accuracy)\u001b[0m\n\u001b[0;32m    289\u001b[0m sigma_search_settings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataloader,\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy,\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_deviation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m    294\u001b[0m }\n\u001b[0;32m    296\u001b[0m mag_eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m--> 297\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[43mpac_bayes_sigma_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msigma_search_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m mag_sigma \u001b[38;5;241m=\u001b[39m pac_bayes_sigma_search(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msigma_search_settings, magnitude_eps\u001b[38;5;241m=\u001b[39mmag_eps)\n\u001b[0;32m    300\u001b[0m distance_vector \u001b[38;5;241m=\u001b[39m calculate_distance_vector(model, init_model)\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\measures_sharpeness.py:156\u001b[0m, in \u001b[0;36mpac_bayes_sigma_search\u001b[1;34m(model, dataloader, accuracy, target_deviation, search_depth, monte_carlo_iter, magnitude_eps, upper, lower, deviation_tolerance, bound_tolerance)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m apply_perturbation(model, sigma, magnitude_eps\u001b[38;5;241m=\u001b[39mmagnitude_eps):\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m--> 156\u001b[0m         perturbed_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perturbed_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         accuracy_samples\u001b[38;5;241m.\u001b[39mappend(perturbed_accuracy)\n\u001b[0;32m    159\u001b[0m deviation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(accuracy_samples) \u001b[38;5;241m-\u001b[39m accuracy)\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\measures_sharpeness.py:110\u001b[0m, in \u001b[0;36mcalculate_perturbed_accuracy\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    109\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 110\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    112\u001b[0m     batch_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\src\\model_constructor.py:29\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m): \u001b[38;5;66;03m#input will be of form (Batch size, 3, 64, 64)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# conv layers\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# flatten to fit fully connected layer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(((\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))) \n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\vasco\\repos\\Robustness-Metrics\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:394\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    396\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from src.measures_sharpeness import calculate_pac_bayes_metrics\n",
    "\n",
    "metrics = calculate_pac_bayes_metrics(trained_model, untrained_model, train_loader, trainer.history['train_acc'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
